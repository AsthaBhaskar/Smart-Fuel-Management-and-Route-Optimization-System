# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIGD0uZpL93R-0bRzBCTj8m6KM7Iwpfi
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
data = pd.read_csv("/content/tugboat_dataset (1).csv")

# Display basic information about the dataset
print(data.head())
print(data.info())
data.isnull()  # Check for missing values

# Print out the column names to see what's available
print(data.columns)

from sklearn.feature_selection import SelectKBest, f_regression
import numpy as np

# Selecting numerical features only for SelectKBest
numerical_features = data.select_dtypes(include=[np.number]).columns.tolist()
numerical_features.remove('Fuel Consumption')

# Separate numerical features from the dataset
X_numerical = data[numerical_features]
y_numerical = data['Fuel Consumption']

# Perform feature selection using SelectKBest with f_regression
select_k_best = SelectKBest(score_func=f_regression, k=10)
select_k_best.fit(X_numerical, y_numerical)

# Get selected feature indices and names
selected_features_indices = select_k_best.get_support(indices=True)
selected_features_kbest = X_numerical.columns[selected_features_indices].tolist()

from sklearn.feature_selection import RFE
from sklearn.ensemble import GradientBoostingRegressor

# Define the model for RFE
model = GradientBoostingRegressor()

# Perform RFE
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(X_numerical, y_numerical)

# Get selected feature indices and names
selected_features_rfe = X_numerical.columns[rfe.support_].tolist()

# Create a list of unique selected features
selected_features = list(set(selected_features_kbest + selected_features_rfe))

# Create a new dataset with the selected features
X_selected = data[selected_features]
y = data['Fuel Consumption']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

from sklearn.model_selection import GridSearchCV
# Define parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}
# Perform Grid Search
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',
                           cv=5,
                           verbose=1)
grid_search.fit(X_train, y_train)

# Best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

from sklearn.ensemble import GradientBoostingRegressor

# Initialize Gradient Boosting model with best parameters
best_gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)

# Fit the model on the training data
best_gb_model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score

# Predict on the test set
y_pred = best_gb_model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score

# Predict on the test set
y_pred = best_gb_model.predict(X_test)

# Compare actual vs predicted
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(comparison.head(10))  # Print first 10 rows for comparison

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
evs = explained_variance_score(y_test, y_pred)

print(f"\nMean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R2): {r2}")
print(f"Explained Variance Score: {evs}")

from sklearn.feature_selection import SelectKBest, f_regression

# Selecting numerical features only for SelectKBest
numerical_features = data.select_dtypes(include=[np.number]).columns.tolist()
numerical_features.remove('Fuel Consumption')

# Separate numerical features from the dataset
X_numerical = data[numerical_features]
y_numerical = data['Fuel Consumption']

# Perform feature selection using SelectKBest with f_regression
select_k_best = SelectKBest(score_func=f_regression, k=10)
select_k_best.fit(X_numerical, y_numerical)

# Get selected feature indices and names
selected_features_indices = select_k_best.get_support(indices=True)
selected_features_kbest = X_numerical.columns[selected_features_indices].tolist()

print(selected_features_kbest)

import numpy as np
import matplotlib.pyplot as plt
import random
import math

# Sample coordinates for points
points = [(-42.98160360109824, 137.55082835153138), (-80.88176054875719, 79.0895385215195),
          (35.36493998446626, 73.0808064214643), (-73.47504774064826, 116.82944872605464)]

# Example function to calculate fuel consumption
def predict_fuel_consumption(point1, point2):
    # Replace with your model's fuel consumption prediction logic
    # Example: Dummy calculation based on Euclidean distance
    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)

# Simulated Annealing Algorithm
def simulated_annealing(points, initial_temp=1000, cooling_rate=0.003, stopping_temp=1e-8, num_iterations=1000):
    num_points = len(points)
    current_state = points[:]
    best_state = current_state[:]

    current_temp = initial_temp

    def cost(route):
        total_cost = 0
        for i in range(num_points - 1):
            total_cost += predict_fuel_consumption(route[i], route[i + 1])
        return total_cost
    def get_neighbor(state):
        neighbor = state[:]
        idx1, idx2 = random.sample(range(num_points), 2)
        neighbor[idx1], neighbor[idx2] = neighbor[idx2], neighbor[idx1]
        return neighbor

    for iteration in range(num_iterations):
        next_state = get_neighbor(current_state)

        current_cost = cost(current_state)
        next_cost = cost(next_state)

        if next_cost < current_cost:
            current_state = next_state[:]
            if next_cost < cost(best_state):
                best_state = next_state[:]
        else:
            delta_cost = next_cost - current_cost
            accept_prob = math.exp(-delta_cost / current_temp)
            if random.random() < accept_prob:
                current_state = next_state[:]

        current_temp *= 1 - cooling_rate

        if current_temp < stopping_temp:
            break

    return best_state, cost(best_state)

# Example usage
optimal_route_sa, optimal_cost_sa = simulated_annealing(points)

print("Simulated Annealing Algorithm's Optimal Route:", optimal_route_sa)
print("Simulated Annealing Algorithm's Optimal Cost (Fuel Consumption):", optimal_cost_sa)

# Plotting the optimal route with coordinates
def plot_route(route):
    latitudes = [point[0] for point in route]
    longitudes = [point[1] for point in route]

    plt.figure(figsize=(10, 6))
    plt.plot(longitudes, latitudes, marker='o', linestyle='-', color='b')

    # Plotting coordinates
    for i, txt in enumerate(route):
        plt.annotate(f'{txt}', (longitudes[i], latitudes[i]), textcoords="offset points", xytext=(0,10), ha='center')

    plt.title('Optimal Route with Least Fuel Consumption (Simulated Annealing)')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.grid(True)
    plt.show()

plot_route(optimal_route_sa)